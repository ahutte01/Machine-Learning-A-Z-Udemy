{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                           Wow... Loved this place.      1\n",
      "1                                 Crust is not good.      0\n",
      "2          Not tasty and the texture was just nasty.      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n"
     ]
    }
   ],
   "source": [
    "# March 8th 2019 \n",
    "\n",
    "# Import libraries \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/Users/amandahutter/Documents/PythonCode/Udemy/MachineLearningA-Z/Part 7 - Natural Language Processing/Section 36 - Natural Language Processing') \n",
    "dataset = pd.read_csv(\"Restaurant_Reviews.tsv\", delimiter = '\\t', quoting = 3)\n",
    "print(dataset.head())\n",
    "\n",
    "# Using CSV file will be difficult because the reviews will have commas in them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the text > Remove irrelevant words that do not help ML algorithm predict if Negative or Positive \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amandahutter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used for regular expressions \n",
    "import re \n",
    "# Import NLTK to assist with removing the non important words \n",
    "import nltk \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow    Loved this place \n",
      "wow    loved this place \n",
      "['wow', 'loved', 'this', 'place']\n",
      "['wow', 'love', 'place']\n",
      "wow love place\n"
     ]
    }
   ],
   "source": [
    "# Remove the non important words \n",
    "review = re.sub(pattern = '[^a-zA-Z]', repl = ' ', string = dataset['Review'][0]) \n",
    "print(review)\n",
    "\n",
    "# Lowercase words \n",
    "review = review.lower()\n",
    "print(review)\n",
    "\n",
    "# Split string into words \n",
    "review = review.split()\n",
    "print(review)\n",
    "\n",
    "# Make instance of this class because it will stem our words together by removing suffixes/prefixes \n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Keep the words in the list that are not in the Stopwords list \n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "print(review)\n",
    "\n",
    "# Convert back to string \n",
    "review = ' '.join(review)\n",
    "print(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crust good', 'tasti textur nasti', 'stop late may bank holiday rick steve recommend love', 'select menu great price', 'get angri want damn pho', 'honeslti tast fresh', 'potato like rubber could tell made ahead time kept warmer', 'fri great', 'great touch', 'servic prompt', 'would go back', 'cashier care ever say still end wayyy overpr', 'tri cape cod ravoli chicken cranberri mmmm', 'disgust pretti sure human hair']\n"
     ]
    }
   ],
   "source": [
    "# Loop through all reviews in your dataset and do above process \n",
    "\n",
    "# corpus refers to the cleaned reviews\n",
    "corpus= []\n",
    "\n",
    "for i in range (0, len(dataset)):\n",
    "    review = re.sub(pattern = '[^a-zA-Z]', repl = ' ', string = dataset['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "print(corpus[1:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pick most popular words to use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape of the sparse matrix (1000, 1565)\n",
      "That is 1565 unique words\n",
      "This is the shape of the new sparse matrix (1000, 1500)\n",
      "We have limited to 1500 most common words.\n",
      "<class 'numpy.ndarray'>\n",
      "Each row is one review, each column is whether a given word from our bag of words is in that review\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec = CountVectorizer()\n",
    "\n",
    "# create sparse matrix containing matrix of features, .toarray() makes this a matrix \n",
    "X = count_vec.fit_transform(corpus).toarray() \n",
    "\n",
    "print('This is the shape of the sparse matrix', X.shape)\n",
    "print(\"That is 1565 unique words\")\n",
    "\n",
    "# Give an upper limit of unique words \n",
    "count_vec = CountVectorizer(max_features = 1500)\n",
    "X = count_vec.fit_transform(corpus).toarray() \n",
    "print('This is the shape of the new sparse matrix', X.shape)\n",
    "print(\"We have limited to 1500 most common words.\")\n",
    "\n",
    "print(type(X))\n",
    "print(\"Each row is one review, each column is whether a given word from our bag of words is in that review\")\n",
    "print(X[1:10, 1:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create Depedent Variable Vector. With known sentiment that are Positive or Negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1000,)\n",
      "These are sentiments:1 is for positive sentiment, 0 is for negative sentiment, from known relationships with words \n",
      "[0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "Y = dataset.iloc[:, 1].values\n",
    "print(type(Y))\n",
    "print(Y.shape)\n",
    "print(\"These are sentiments:1 is for positive sentiment, 0 is for negative sentiment, from known relationships with words \")\n",
    "print(Y[1:25,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a Classification Model to use, test for false positives\n",
    "\n",
    "### The most common classification models to use are:\n",
    "### - Naive Bayes\n",
    "### - Decision Tree \n",
    "### - Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions based on Y Test characteristics: \n",
      " [1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0\n",
      " 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1]\n",
      "The actual Y Test: \n",
      " [0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
      " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1]\n",
      "The confusion matrix: \n",
      " [[55 42]\n",
      " [12 91]]\n",
      "Accuracy is: 0.73\n",
      "Precision is: 0.5670103092783505\n",
      "Recall is: 0.8208955223880597\n",
      "Fscore is: 0.6707317073170731\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fit Naive Bayes Classifier to the training data set  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_Train, Y_Train)\n",
    "\n",
    "# Predict the DV Test set using the classifer \n",
    "Y_Pred = classifier.predict(X_Test)\n",
    "print(\"Predictions based on Y Test characteristics: \\n\", Y_Pred)  \n",
    "print(\"The actual Y Test: \\n\", Y_Test)\n",
    "\n",
    "# Making the confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true = Y_Test, y_pred = Y_Pred) # create an instance of confusion matrix class \n",
    "print(\"The confusion matrix: \\n\", cm)\n",
    "\n",
    "TP = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "TN = cm[1, 1]\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "print(\"Accuracy is:\", Accuracy)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "print(\"Precision is:\", Precision)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "print(\"Recall is:\", Recall)\n",
    "\n",
    "FScore = (2*Precision*Recall/(Precision + Recall))\n",
    "\n",
    "print(\"Fscore is:\", FScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amandahutter/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions based on Y Test characteristics: \n",
      " [0 0 1 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0\n",
      " 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1\n",
      " 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0\n",
      " 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0]\n",
      "The actual Y Test: \n",
      " [0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
      " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1]\n",
      "[[74 23]\n",
      " [35 68]]\n",
      "Accuracy is: 0.71\n",
      "Precision is: 0.7628865979381443\n",
      "Recall is: 0.6788990825688074\n",
      "Fscore is: 0.7184466019417477\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fit \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_Train, Y_Train)\n",
    "\n",
    "# Predict \n",
    "Y_Pred = classifier.predict(X_Test)\n",
    "print(\"Predictions based on Y Test characteristics: \\n\", Y_Pred)  \n",
    "print(\"The actual Y Test: \\n\", Y_Test)\n",
    "\n",
    "\n",
    "# Confusion Matrix \n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_Test, Y_Pred)\n",
    "print(cm)\n",
    "\n",
    "TP = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "TN = cm[1, 1]\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "print(\"Accuracy is:\", Accuracy)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "print(\"Precision is:\", Precision)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "print(\"Recall is:\", Recall)\n",
    "\n",
    "FScore = (2*Precision*Recall/(Precision + Recall))\n",
    "\n",
    "print(\"Fscore is:\", FScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions based on Y Test characteristics: \n",
      " [0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
      " 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0]\n",
      "The actual Y Test: \n",
      " [0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
      " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1]\n",
      "[[87 10]\n",
      " [46 57]]\n",
      "Accuracy is: 0.72\n",
      "Precision is: 0.8969072164948454\n",
      "Recall is: 0.6541353383458647\n",
      "Fscore is: 0.7565217391304349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amandahutter/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Split the data \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.20, random_state = 0 )\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_Train, Y_Train)\n",
    "\n",
    "# Predict based on the X Test set that was witheld from the fitted data \n",
    "Y_Pred = classifier.predict(X_Test)\n",
    "print(\"Predictions based on Y Test characteristics: \\n\", Y_Pred)  \n",
    "print(\"The actual Y Test: \\n\", Y_Test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_Test, Y_Pred)\n",
    "print(cm)\n",
    "\n",
    "TP = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "TN = cm[1, 1]\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "print(\"Accuracy is:\", Accuracy)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "print(\"Precision is:\", Precision)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "print(\"Recall is:\", Recall)\n",
    "\n",
    "FScore = (2*Precision*Recall/(Precision + Recall))\n",
    "\n",
    "print(\"Fscore is:\", FScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions based on Y Test characteristics: \n",
      " [0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 0 0\n",
      " 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
      " 0 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0]\n",
      "The actual Y Test: \n",
      " [0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1\n",
      " 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1\n",
      " 1 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1]\n",
      "[[82 15]\n",
      " [48 55]]\n",
      "Accuracy is: 0.685\n",
      "Precision is: 0.845360824742268\n",
      "Recall is: 0.6307692307692307\n",
      "Fscore is: 0.7224669603524229\n"
     ]
    }
   ],
   "source": [
    "# Split the data \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.20, random_state = 0 )\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 0)\n",
    "classifier.fit(X_Train, Y_Train)\n",
    "\n",
    "# Predict based on the X Test set that was witheld from the fitted data \n",
    "Y_Pred = classifier.predict(X_Test)\n",
    "print(\"Predictions based on Y Test characteristics: \\n\", Y_Pred)  \n",
    "print(\"The actual Y Test: \\n\", Y_Test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_Test, Y_Pred)\n",
    "print(cm)\n",
    "\n",
    "TP = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "TN = cm[1, 1]\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "print(\"Accuracy is:\", Accuracy)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "print(\"Precision is:\", Precision)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "print(\"Recall is:\", Recall)\n",
    "\n",
    "FScore = (2*Precision*Recall/(Precision + Recall))\n",
    "\n",
    "print(\"Fscore is:\", FScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In future, try these: \n",
    "# CART - sklearn is an optimized version of this \n",
    "# C5.0 - extension of the ID3 algorithm, same as C4.5\n",
    "# Max Entropy - depreciated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
